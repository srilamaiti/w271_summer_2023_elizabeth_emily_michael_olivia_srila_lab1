---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Michael Denton, Srila Maiti, Olivia Pratt, Emily Robles, Elizabeth Willard"
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
header-includes:
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(psych)
library(knitr)
library(stargazer)
library(car)
library(gridExtra)
library(tseries)
```
\newpage

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
\end{abstract}
```
# Introduction

## Research question

\newpage
# Data (20 points)

## Description

The data-set for analyzing the risk of O-ring failure in the Challenger Space Shuttle flight consists of twenty-three (23) out of the twenty-four (24) prior launches (one launch data is not available because the motors were lost at sea). The flight number increases by increment of one and corresponds to each launch chronologically. The data is generated by the recovery of the solid rocket motors and subsequent inspection of each component. 

We assume that each flight is independent and identically distributed (IID) for the purposes of this analysis. We are able to make this assumption because each flight was, theoretically, in the same configuration with O-rings manufactured, spec'd, and installed in the same way. Without the assumption of independence, our ability to infer meaning from the variables would be suspect and we would not be able to conclude with certainty from the statistical analysis. However, the reality is that certain tolerances exist for everything, and parts may not be completely the same with each flight. In fact, the original author (Dalal et al) also expressed that the engineering knowledge of part tolerances was not necessarily consistent with the sampling distribution and conducted a thorough analysis to alleviate concerns.

In addition to the Flight number, variables for launch temperature (**Temperature (°F)**), leak test pressure (**Pressure (psi)**), and the number of O rings having some thermal distress (**O-Ring**) are recorded.  The first three values of the data set are presented in **Table 1**.
```{r echo=FALSE, warning=FALSE, message=FALSE}

df <- read.csv('~/school/MIDS-271/mids-w271-oliviapratt/data/challenger.csv')

# Add some other columns
#df$Mechanism <- ifelse(df$O.ring==1,"Erosion",ifelse(df$O.ring==2,"Blowby",""))
#df$Comment <- ifelse(df$Flight==14, "Secondary O-ring Distress Noted", "")

#head(df)
#describe(df)

```
```{r table1, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring")

stargazer(head(df[cols],3),type='latex', summary=FALSE, rownames=FALSE, 
          title='First Three Data Points of the Measured Flights Prior to Challenger Disaster',
          column.labels = c("Flight Number", "Temperature (°F)", "Pressure (psi)", "Primary O-Ring Failure Mechanism","Additional Comments"), 
          header=FALSE)

```
## Key Features

There are peculiarities in the data-set corresponding to the available features. The first is, over time the leak test pressure changes. Dalal et al. explain this was due to finding that the putty sealing the O-rings could sustain 50 psi, so leak tests were increased to 100 and 200 psi to properly test for O-ring capability. It's also noted, however, that this increased leak pressure may have contributed to blow holes, causing erosion in the putty. 

The temperatures gathered that up to the fatal Challenger flight are key findings. The Challenger was scheduled to launch when the temperature was around freezing (31 °F). As seen in **Table 2**, the minimum temperature experienced prior to the launch failure was 53°F. This particular launch corresponded to secondary O-ring distress, with the mechanism reported as primary O-ring blow by. The presence of secondary O-ring failure is critical because it shows that this condition could lead to failure in both O rings and thus ex filtration of the engine gases.
```{r table2, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring")

stargazer(df[cols], type='latex', summary=TRUE, header=FALSE,
          title="Baseline Statistics of Prior Flights to Challenger Disaster")

```
The temperature effect observed in the data set is confirmed in **Figure 1**, which is a panel figure comparing the features and their reported Pearson correlation coefficients. It's shown that the correlation between the the temperature component and the O-ring failures is -0.51, meaning that lower temperatures were considered to result in more O-ring failures. Also note the correlation between *Pressure* and *Flight*, which is explained by the change in procedure over time. To a lessor degree, there also exists a correlation between *Pressure* and *O-ring* which may indicate a potential pressure effect to explore. This is also supported by the theory discussed above that high leak test pressure can lead to putty erosion. However, Dalal et al largely dismissed *Pressure* as an important explanatory variable.
```{r pairsfig, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Panel Plot of Flight Variables with Pearson Correlation'}
cols <- c("Flight","Temp","Pressure","O.ring")

pairs.panels(
  df[cols], 
  method = "pearson", # correlation method
  hist.col = "#00AFBB",
  density = TRUE,  # show density plots
  ellipses = TRUE # show correlation ellipses
)
```
# Analysis

## Reproducing Previous Analysis

We found negative correlation between *Temp* and 
*O-ring* and positive correlation between *Pressure* and *O-ring*. Because of this, we 
include both explanatory features in our first logistic regression model.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$ and leak test pressure as $p_i$. 
With that, the binomial logistic regression model will be formulated as:
$$
logit \left( \pi_i \right)  = log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 t_i + \beta_2 p_i
$$
```{r binomial logistic regression model, message=FALSE, warning=FALSE}
# Binomial Logit Model
model_binomial_lr <- glm(formula = O.ring / Number ~ Temp + Pressure, 
               data = df, family = binomial(link = logit), weights = Number)
# Confidence interval
model_binomial_lr_confint <- confint(model_binomial_lr)
```
For the binomial logistic regression model, we have included two explanatory 
numeric variables *Temp* and *Pressure*. They are included in "as-is" form, 
without any transformation. The outcome variable is a proportion, derived as 
the ratio of number O-ring failures to total number of O-rings present, which is 6.
The estimated model is shown in **Table 3** along with 95% confidence intervals and presented below:

$$ logit\left(\hat{\pi}\right)= 2.520 - 0.098\text{Temp} + 0.008\text{Pressure} $$
```{r lrt, message=FALSE, warning=FALSE, echo=FALSE}
# Likelihood Ratio Test
anova_model_binomial_lr <- Anova(model_binomial_lr, test = 'LR')

anova_model_binomial_lr_chisq_temp <- round(anova_model_binomial_lr$`LR Chisq`[1],2)
anova_model_binomial_lr_chisq_press <- round(anova_model_binomial_lr$`LR Chisq`[2],2)

anova_model_binomial_lr_pr_temp <- round(anova_model_binomial_lr$`Pr(>Chisq)`[1],2)
anova_model_binomial_lr_pr_press <- round(anova_model_binomial_lr$`Pr(>Chisq)`[2],2)
```
We can also conduct a Likelihood Ratio test using the *Anova* function from the *car* package in R to determine statistical importance of each explanatory variable. This test yields a $\chi^2$ value of `r anova_model_binomial_lr_chisq_temp` for *Temperature*, corresponding to a P value of `r  anova_model_binomial_lr_pr_temp`, which is significant. For *Pressure*, the test yields a $\chi^2$ value of `r anova_model_binomial_lr_chisq_press` corresponding to a P value of `r  anova_model_binomial_lr_pr_press`, which is not significant.

```{r invodds, message=FALSE, warning=FALSE, echo=FALSE}
# Inverse odds ratio for binomial model
# round(1/exp(summary(model_binomial_lr)$coefficients),3)
```
### Major Observations:

- In the results of the likelihood ratio test, temperature appears to 
be statistically significant predictor of the proportion of O-ring failures. 
It also shows the inverse relationship between O-ring failure proportion 
and the temperature. 
- The pressure variable does not appear to be a statistically significant predictor of the 
o-ring failure proportion.
- The coefficient of $t_i$ is estimated as
`r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,1]` 
with the 95% Wald confidence interval of `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,2]` to `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,3]`, indicating that 
the decrease of the launch temperature would cause an increase on the odds 
for an O-ring to fail. 
- In the Anova test, involving feature variable of *Temp*, according to 
$H_0 : \beta_1 = 0$ and $H_\alpha : \beta_1 \neq 0$. 
The LRT statistic for *Temp* is `r Anova(model_binomial_lr, test = 'LR')[1,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[1,3]`. 
- Using the Type I Error rate $alpha = 0.05$, we reject the null hypothesis and 
accept that the feature *Temp* is important to be included in the model with
other feature variable *Pressure* is included the model. 
- A 10°F temperature drop increases the odds O-ring failure by approximately 
`r abs(round(floor(exp(-10*coef(model_binomial_lr)['Temp'])) - exp(-10*coef(model_binomial_lr)['Temp']), 2)) * 100`%. 

The test of *Pressure* with $H_0 : \beta_2 = 0$ vs. $H_\alpha : \beta_2 \neq 0$, 
produced an LRT statistic of `r Anova(model_binomial_lr, test = 'LR')[2,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[2,3]`. Using the 
Type I Error rate $alpha = 0.05$, we fail to reject the null hypothesis. 
Therefore, there is a lack of evidence to include the feature variable *Pressure* 
in the model.

Dalal et al. could have also excluded `Pressure` for this reason. Additionally, `Pressure` does not add much systematic variation to the model. `Pressure` has unique values `r unique(df$Pressure)`, 
meaning the data available is very small and not representative. For this reason, we also determined to exclude `Pressure` from our model.

It is worth noting that potential problems could arise by excluding *Pressure* from
the model. If more data was available, pressure along with the interaction with
temperature might be helpful for further analysis. If pressure is correlated with
temperature and O-ring failure, by removing it we could have introduced
omitted variable bias.

## Confidence Intervals

In our next model, we are only including temperature as our explanatory variable.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$. With that, the model will be formulated as:
$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i
$$
```{r, model temp, message=FALSE, warning=FALSE}
# Binomial Logit Model with just temperature
model_binomial_lr_temp <- glm(formula = O.ring / Number ~ Temp, 
               data = df, family = binomial(link = logit), weights = Number)
# Confidence interval
model_binomial_lr_temp_confint <- confint(model_binomial_lr_temp)
```
This binomial logistic regression model is estimated in **Table 3** and shown below:
$$ logit\left(\hat{\pi}\right)= 5.085 - -0.116\text{Temp}$$
```{r lrt2, message=FALSE, warning=FALSE, echo=FALSE}
# Likelihood Ratio Test
model_binomial_lr_temp_lr <- Anova(model_binomial_lr_temp, test = 'LR')

anova_model_binomial_lr_temp_chisq_temp <- round(model_binomial_lr_temp_lr$`LR Chisq`[1],2)

anova_model_binomial_lr_temp_pr_temp <- round(model_binomial_lr_temp_lr$`Pr(>Chisq)`[1],2)
```
Again, we perform a likelihood ratio test using *Anova* and find $\chi^2$ value of `r anova_model_binomial_lr_temp_chisq_temp` corresponding to a P value of `r anova_model_binomial_lr_temp_pr_temp`, which is significant.

Finally, in our third logistic regression model, we use temperature and it's quadratic
term as explanatory features to predict the probability of O-ring failure.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$. 
Therefore, the model is formulated as:

$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i + \beta_2 t_i ^ 2
$$
```{r, model with polynomial temp, message=FALSE, warning=FALSE}
# Binomial logistic regression with quadratic temperature term
model_binomial_lr_temp_poly <- glm(formula = O.ring / Number ~ Temp + I(Temp ^ 2), 
               data = df, family = binomial(link = logit), weights = Number)

# Confidence interval
model_binomial_lr_temp_poly_confint <- confint(model_binomial_lr_temp_poly)
```
```{r lrt3, message=FALSE, warning=FALSE, echo=FALSE}
# Log likelihood test
model_binomial_lr_temp_poly_lr <- Anova(model_binomial_lr_temp_poly, test = 'LR')

anova_model_binomial_lr_temp_poly_chisq_temp <- round(model_binomial_lr_temp_poly_lr$`LR Chisq`[1],2)
anova_model_binomial_lr_temp_poly_chisq_tempsq <- round(model_binomial_lr_temp_poly_lr$`LR Chisq`[2],2)

anova_model_binomial_lr_temp_poly_pr_temp <- round(model_binomial_lr_temp_poly_lr$`Pr(>Chisq)`[1],2)
anova_model_binomial_lr_temp_poly_pr_tempsq <- round(model_binomial_lr_temp_poly_lr$`Pr(>Chisq)`[2],2)
```
A likelihood ratio test is again ran on this polynomial model, with $\chi^2$ value of `r anova_model_binomial_lr_temp_poly_chisq_temp` for *Temperature* and P value of `r anova_model_binomial_lr_temp_poly_pr_temp`. For the quadratic term, there is a $\chi^2$ value of `r anova_model_binomial_lr_temp_poly_chisq_tempsq` for *Temperature* and P value of `r anova_model_binomial_lr_temp_poly_pr_tempsq`.

A comparison of all three models is presented below, in **Table 3**.
```{r models, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
cclist <- list(model_binomial_lr_confint, model_binomial_lr_temp_confint, model_binomial_lr_temp_poly_confint)

stargazer(
  model_binomial_lr, 
  model_binomial_lr_temp, 
  model_binomial_lr_temp_poly, 
  type='latex', 
  title='Comparison of Statistical Models to Predict Number of Failures', 
  header=FALSE,
  ci.custom=cclist,
  column.labels=c('Temp + Press','Temp Only','Temp w/ Quadratic')
)
```
We aimed to see whether the quadratic temperature has any effect on predicting
O-ring failure, however it does not have significance. So we are
not going to use this quadratic temperature feature in our model.

### Probability Figures

Above, we predicted the probability of failure using the second model (just *Temp*), and calculated the confidence intervals. Here we calculate the expected number of O-ring failures and plot that against temperature, as shown below in **Figure 2**:
```{r fig2, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Predicted Probability and Expected Number of O Ring Failures'}
t <- seq(31,81,1)
alpha=0.05
model_predict <- predict(object=model_binomial_lr_temp, 
                         newdata=data.frame(Temp=t), 
                         type='link', 
                         se=T)
CI_lower_linear <- model_predict$fit + qnorm(p=alpha/2)*model_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_predict$fit + qnorm(p=1-alpha/2)*model_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))

par(mfrow=c(1,2), oma=c(2,0,2,0))

##### pi with CI vs temp
plot(df$Temp, df$O.ring/df$Number,
     xlab= "Temperature (°F)",
     ylab=expression(pi),
     pch=20, cex=1.5,
     xlim=c(31,81),
     ylim=c(0,1), 
     sub="o-ring failure probability vs temp")
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
#title(expression(bold("o-ring failure probability vs temp")), adj=0)

##### Expected number of failures vs temp
plot(df$Temp, df$O.ring,
     xlab="Temperature (°F)",
     ylab="Expected Number\nof O-ring failures",
     pch=20, cex=1.5, 
     col="black",
     xlim=c(31,81),
     ylim=c(0,6),
     sub="Estimated o-ring failure vs temp")
## Binomial betas
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```
The confidence band is wider at temperatures lower than `r min(df$Temp)`°F 
because there are no observations below `r min(df$Temp)`°F.

### Probability of failure at 31°F

Using the models generated above, we predict the probability of a failure occurring on the day of the Challenger disaster, when launch temperature was estimated at 31°F.

```{r, extrapolate at 31 deg F}
# Data to extrapolate
predict.data <- data.frame(Temp = 31)
# Predict surface/link
predict.linear <- predict(object = model_binomial_lr_temp,
                          newdata = predict.data, type = "link")
# Predict response 
predict.pi <- predict(object = model_binomial_lr_temp, newdata = predict.data,
                      type = "response")
pfail_31 <- data.frame(estimate=predict.pi, lower=CI_lower_pi[[1]], 
                       upper=CI_higher_pi[[1]])
```
The probability of O-ring failure at 31°F is `r round(predict.pi,3)` with a very wide 
confidence interval of `r round(c(CI_lower_pi[[1]], CI_higher_pi[[1]]),3)`. These are shown in **Table 4**.

```{r warning=FALSE, echo=FALSE, message=FALSE, results='asis'}
# Confidence interval for 31deg was computed above in part (c)
stargazer(head(pfail_31), summary=FALSE, header=FALSE, title='Estimated probability of failure with confidence bounds of fatal Challenger Flight.')
```
In order to infer the probability of O-ring failure at 31°F, we need to make 
the assumption that there is a linear relationship between the log odds of failure 
of an O-ring and Temperature. Because we use the binomial model, we do not meet 
the independence assumption. We also need to assume the model is built 
off a representative data set including records with both high and low 
temperatures, high and low pressure levels, and a similar data range is used for both
model training and inference. Otherwise the results will be uncertain and we can 
not trust the model's predictions.

## Bootstrap Confidence Intervals (30 points)

For the parametric bootstrap procedure, we have performed the below steps:

- Setting a seed to make the results reproducible.
- Estimate the proportion of O-ring failure using the estimated coefficients.
- Instantiate a result data frame to store the results at 31°F and 72°F.
- Enumerate iterations and follow the steps:
   1. Resample original data-set with replacement to create a new data-set 
     $d$ of size 23.
   2. A vector of size 23 with outcome variable $O.ring2$ is generated using 
      rbinom function and using the sampled data and estimated $\hat\pi$ values.
   3. A new binomial logistic regression model is fitted with the resampled 
      dataset $d$ and outcomes $O.ring2$.
   4. The predictions for 31°F and 72°F are found and the estimated probabilities 
      are saved to $results$.
   5. Finally, the 90% confidence interval is derived from the predictions for 31°F and 72°F 
      and stored in $results$.

```{r, parametric bootstrap, message=FALSE, warning=FALSE, echo=FALSE}
# Set a seed
set.seed(123)
# Use pi estimated from the model
z = model_binomial_lr_temp$coefficients["(Intercept)"] + 
    model_binomial_lr_temp$coefficients["Temp"] * df$Temp
pi = exp(z)/(1+exp(z))
# Save the pi array to the dataframe
df$O.ring.pi <- pi
# Dataframe to populate with results
results <- data.frame(pred.31 = numeric(), pred.72 = numeric())
iterations = 10000
for (s in 1:iterations){
  I.sample <- sample(x = 1:nrow(df), size = 23, replace = T)
  # Populate d with samples
  d <- df[I.sample,]  
  # Simulate outcomes using rbinom
  # n : sample size, size : number of trials and prob is the probability
  O.ring2 <- rbinom(n=23, size=6, prob=d$O.ring.pi) 
  # Stacking the simulated o.ring value with the sample data
  d <- data.frame(d, O.ring2)
  # Estimate model with rbinom bootstrap outcomes using the sampled data and sampled data
  mod <- glm(O.ring2/Number ~ Temp, family = binomial(link = logit), 
             weights = Number, data = d)
  # Estimate confidence interval for temp = 31
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data, type = "response")
  # Estimate confidence interval for temp = 72
  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data, type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}
```
```{r quantiles, message=FALSE, echo=FALSE, warning=FALSE, results='asis'}
CI.31 <- quantile(results[,1], probs=c(0.05, 0.95))
CI.72 <- quantile(results[,2], probs=c(0.05, 0.95))
quantiles_df <- data.frame(temp=c(31,72), lower.CI=c(CI.31[1],CI.72[1]), upper.CI=c(CI.31[2],CI.72[2]))
stargazer(quantiles_df, summary=FALSE, header=FALSE, type='latex', title='Parametric Bootstrap Analysis Results')
```
```{r fig3, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Predicted Probability and Expected Number of O Ring Failures'}
# Plot histograms of confidence intervals
par(mfrow=c(1,2),mar=c(5,4,4,2))
# Histogram for temp = 31
hist(results[,1], breaks=50, col="lightskyblue3", xlab=expression(pi), xlim=c(0,1), main=' ')
mtext('(a) Probability of Failure at 31 °F', side = 3, padj = -2.5)
# Histogram for temp = 72
hist(results[,2], breaks=10, col="orangered", xlab=expression(pi), xlim=c(0,1), main=' ')
mtext('(b) Probability of Failure at 72 °F', side = 3, padj = -2.5)
```
The parametric bootstrap method to estimate confidence interval also shows that 
the confidence interval for 31°F is quite wide at `r round(CI.31[1],4)` and 
`r round(CI.31[2],4)`, due to the lack of data points at low temperatures. 
The confidence interval estimated for 72°F is much tighter, at `r round(CI.72[1],4)` 
and `r round(CI.72[2],4)`. 

Using `r iterations` iterations, through parametric bootstrap process, we have
generated the histogram of their probability of failures. The confidence intervals 
were taken as the 5th and 95th percentile of the distributions. The probability 
of failures for 31°F took on the full range of 0 and 1, and had a left skew 
towards 1. In contrast, the probability of failures for 72°F were more 
concentrated at 0. 

## Alternative Specification 

A linear model was built to serve the same purpose of the logistic regression models and predict the number of O-ring failures.

```{r, linear model, message=FALSE, warning=FALSE}
# Build Linear Model
model.linear <- lm(O.ring/Number ~ Temp, data = df)
```
```{r message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
# Summary
#stargazer(model.linear, type="latex", header=FALSE, title='Linear Model to Predict Number of Failures')
```
```{r fig4, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Linear Regression Model'}
#par(mfrow=c(2,2))
#plot(model.linear)
```
```{r fig5, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Linear Regression Model Residual Plot'}
#hist(model.linear$residuals)
```
```{r,echo=FALSE, warning=FALSE, message=FALSE}
jarque.bera.p.value <- round(jarque.bera.test(model.linear$residuals)[3][[1]],5)
shapiro.test.p.value <- shapiro.test(sample(model.linear$residuals, size = 5000, replace = TRUE))[2][[1]]
```
Estimated linear model is given below:
$$ \cfrac{O.ring}{Number} = .616 - 0.008Temp$$
**Observations for the linear model:**

- O-ring failure is inversely related with temperature, that is, with temperature 
  increases, the risk of O-ring failure decreases. 
- For each unit increase in temperature, the expected proportion of 
  O-ring failure drops by `r coef(summary(model.linear))["Temp", 1]` with a 
  standard error of  `r coef(summary(model.linear))["Temp", 2]`, and *p*-value of 
  `r coef(summary(model.linear))["Temp", 4]`. 
    
*Linear regression model assumptions:*  

- *IID*
For our analysis, we utilized the entire population of the available data-set
to examine the Challenger disaster on January 28, 1986. We do not have any evidence that one 
particular flight record is related to other flight record, thus this data is independent and identically distributed.

- *No perfect colinearity*
Our model contains only one explanatory variable, so there is no perfect colinearity.

- *Linear Conditional Expectations*
Based on the given data set, and plot for fitted vs residual, we do not see a 
linear relationship. This assumption is not satisfied.

- *Homoscadasticity*
Homoscadasticity assumption is to have constant residual variance across the 
range of explanatory variables. The ocular test shows that this assumption 
is not satisfied. 

- *Normally Distributed Errors*
The relationship between explanatory and the mean of outcome variable is linear. 
Based on the plot, we see that the errors are not normally distributed as 
the tail is significantly thinner than a normal distribution. 
Both Jarque-Bera test value `r jarque.bera.p.value` and Shapiro-Wilk test value 
`r shapiro.test.p.value` denotes that the distribution on the residuals 
in question is significantly different from a normal distribution because of the 
presence of outliers as seen the residual vs fitted plot.

This linear model formulation also expects the proportion of o-ring failure is 
linearly related to the explanatory variables for all of their possible values,
which is not a valid assumption either.

We would choose binomial logistic regression model over the linear regression 
model for the below reasons:-

- The binomial logistic regression translates the problem statement in a clear
  proportion question. The response is the proportion of O-ring failure 
  (e.g. a proportion from 0 to 1).
- The linear model assumptions do not hold true here. Therefore, the linear 
  regression model is not applicable for this problem. On the other hand, 
  using the logistic regression, we do not need to assume a linear relationship 
  between the explanatory variable and response variable, normally distributed 
  residuals and residuals to have constant variance. As a result, binomial 
  logistic regression will be a better choice than linear regression.

# Conclusions (10 points)

Interpret the main result of your preferred model in terms of both odds and probability of failure. Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.
