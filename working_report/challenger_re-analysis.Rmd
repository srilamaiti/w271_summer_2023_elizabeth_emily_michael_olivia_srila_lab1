---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Michael Denton, Srila Maiti, Olivia Pratt, Emily Robles, Elizabeth Willard"
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(psych)
library(knitr)
library(stargazer)
library(car)
library(gridExtra)
library(tseries)
```


\newpage

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
\end{abstract}
```
# Introduction

## Research question

\newpage
# Data (20 points)

## Description

The data-set for analyzing the risk of O-ring failure in the Challenger Space Shuttle flight consists of twenty-three (23) out of the twenty-four (24) prior launches (one launch data is not available because the motors were lost at sea). The flight number increases by increment of one and corresponds to the incremental number of the flight (increases over date-time). The data is generated by the recovery of the solid rocket motors and subsequent inspection of each component. 

We assume that each flight is independent and identically distributed (IID) for the purposes of this analysis. We are able to make this assumption because each flight was, theoretically, in the same configuration with O-rings manufactured, spec'd and installed in the same way. Without the assumption of independence, our ability to infer meaning from the variables would be suspect and we would not be able to conclude with certainty from the statistical analysis. However, the reality is that certain tolerances exist for everything, and parts may not be completely the same with each flight. In fact, the original author (Dalal et al) also expressed that the engineering knowledge of part tolerances was not necessarily consistent with the sampling distribution and conducted a thorough analysis to allevieate concerns.

In addition to the Flight number, variables for launch temperature (**Temperature (°F)**), leak test pressure (**Pressure (psi)**), and the number of O rings having some thermal distress (**O-Ring**) are recorded.  The first and last values data set is presented in **Table 1**.


```{r echo=FALSE, warning=FALSE, message=FALSE}

df <- read.csv('~/271/summer_23_central/Labs/Lab_1/data/raw/challenger.csv')

# Add some other columns
#df$Mechanism <- ifelse(df$O.ring==1,"Erosion",ifelse(df$O.ring==2,"Blowby",""))
#df$Comment <- ifelse(df$Flight==14, "Secondary O-ring Distress Noted", "")

#head(df)
#describe(df)

```

```{r table1, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring")

stargazer(head(df[cols]),type='latex', summary=FALSE, rownames=FALSE, 
          title='First Five Data Points of the Measured Flights Prior to Challenger Disaster',
          column.labels = c("Flight Number", "Temperature (°F)", "Pressure (psi)", "Primary O-Ring Failure Mechanism","Additional Comments"), 
          header=FALSE)

```

## Key Features

Evident in in the dataset are some peculiarities corresponding to the available features. The first is that, over time, the leak test pressure changes. Dalal et al. explain that this was due to finding that the putty by itself could sustain 50 psi, so the leak test was increased to 100, and then to 200 psi to properly test for O ring capability. It's also noted, however, that this increased leak pressure may have contributed to blow holes, which can lead to erosion in the putty. 

Another major finding is the temperatures that were experienced leading up to the fatal Challenger flight, which was scheduled to launch with a temperature hovering around freezing (31 °F). As seen in **Table 2** explicitly, the minimum temperature experienced prior to the launch failure was 53°F. It's also seen in **Table 1** that this particular launch corresponded to secondary O ring distress and and the mechanism was reportedly primary O ring blow by. The presence of secondary O-ring failure is critical because it shows that this condition could lead to failure in both O rings and thus penetration of the engine gases.


```{r table2, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring")

stargazer(df[cols], type='latex', summary=TRUE, header=FALSE,
          title="Baseline Statistics of Prior Flights to Challenger Disaster")

```

The temperature effect alluded to in the data set is confirmed in **Figure 1**, which is a panel figure comparing the features and their reported Pearson correlation coefficients. It's shown that the correlation between the the temperature component and the O-ring failures is -0.51, meaning that lower temperatures were considered to result in more O-ring failures. Also note the correlation between *Pressure* and *Flight*, which is easily explained by the change in procedure over time. To a lessor degree also exists a correlation between *Pressure* and *O ring* which may indicate a potential pressure effect to explore. This is also supported by the theory discussed above that high leak test pressure can lead to putty erosion. However, Dalal et al largely dismissed *Pressure* as an important explanatory variable.

```{r pairsfig, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Panel Plot of Flight Variables with Pearson Correlation'}
cols <- c("Flight","Temp","Pressure","O.ring")

pairs.panels(
  df[cols], 
  method = "pearson", # correlation method
  hist.col = "#00AFBB",
  density = TRUE,  # show density plots
  ellipses = TRUE # show correlation ellipses
)

```

# Analysis

## Reproducing Previous Analysis

From the exploratory data analysis, we found negative correlation between *Temp* and 
*O-ring* and positive correlation between *Pressure* and *O-ring*.
We want to include both explanatory features in our first logistic regression model.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$ and leak test pressure as $p_i$. 
With that, the binomial logistic regression model will be formulated as:-

$$
logit \left( \pi_i \right)  = log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 t_i + \beta_2 p_i
$$

```{r binomial logistic regression model, message=FALSE, warning=FALSE}
# Binomial Logit Model
model_binomial_lr <- glm(formula = O.ring / Number ~ Temp + Pressure, 
               data = df, family = binomial(link = logit), weights = Number)

# Confidence interval
model_binomial_lr_confint <- confint(model_binomial_lr)
```

For the binomial logistic regression model, we have included two explanatory 
numeric variables *Temp* and *Pressure*. We have included them in the as-is form 
without any transformation. The outcome variables is a proportion, derived as 
the ratio of number o-ring failure to total number of o-ring present, which is 6.
The estimated model is shown in **Table 3** along with 95% confidence intervals and presented below:

$$ logit\left(\hat{\pi}\right)= 2.520 - 0.098\text{Temp} + 0.008\text{Pressure} $$

```{r lrt, message=FALSE, warning=FALSE, echo=FALSE}
# Likelihood Ratio Test
anova_model_binomial_lr <- Anova(model_binomial_lr, test = 'LR')

anova_model_binomial_lr_chisq_temp <- round(anova_model_binomial_lr$`LR Chisq`[1],2)
anova_model_binomial_lr_chisq_press <- round(anova_model_binomial_lr$`LR Chisq`[2],2)

anova_model_binomial_lr_pr_temp <- round(anova_model_binomial_lr$`Pr(>Chisq)`[1],2)
anova_model_binomial_lr_pr_press <- round(anova_model_binomial_lr$`Pr(>Chisq)`[2],2)

```
We can also conduct a Likelihood Ratio test using the *Anova* function from the *car* package in R to determine statistical importance of each explanatory variable. That test, for *Temperature* yields a $\chi^2$ value of `r anova_model_binomial_lr_chisq_temp` corresponding to a P value of `r  anova_model_binomial_lr_pr_temp`, which is significant. For *Pressure*, the test yields a $\chi^2$ value of `r anova_model_binomial_lr_chisq_press` corresponding to a P value of `r  anova_model_binomial_lr_pr_press`, which is not significant.

```{r invodds, message=FALSE, warning=FALSE, echo=FALSE}
# Inverse odds ratio for binomial model
# round(1/exp(summary(model_binomial_lr)$coefficients),3)
```
### Major Observations:

a. In the results of the likelihood ratio test, we see that temperature appears to 
be statistically significant predictor on the proportion of o-ring failures. 
It is also showing the inverse relationship between o-ring failure proportion 
and the temperature. 

b. The pressure variable does not appear to be statistically significant predictor 
for predicting of o-ring failure proportion.

c. The coefficient of $t_i$ is estimated to be 
`r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,1]` 
with the 95% Wald confidence interval of `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,2]` to `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,3]`, indicating that 
the decrease on the launch temperature would cause the increase on the odds 
for an O-ring to fail. 

d. In Anova test, involving feature variable of *Temp*, according to 
$H_0 : \beta_1 = 0$ and $H_\alpha : \beta_1 \neq 0$. 
The LRT statistic for *Temp* is `r Anova(model_binomial_lr, test = 'LR')[1,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[1,3]`. 

e. Using the Type I Error rate $alpha = 0.05$, we reject the null hypothesis and 
accept that the feature *Temp* is important to be included in the model with
other feature variable *Pressure* is included the model. 

f. For the test of *Pressure* with $H_0 : \beta_2 = 0$ vs. $H_\alpha : \beta_2 \neq 0$, 
The LRT statistic of *Pressure* is `r Anova(model_binomial_lr, test = 'LR')[2,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[2,3]`. Using the 
Type I Error rate $alpha = 0.05$, we fail to reject the null hypothesis. 
Therefore, there is a lack of evidence to include the feature variable *Pressure* 
in the model. 

g. With 10 deg F temperature drop increases the odds o-ring failure by approximately 
`r abs(round(floor(exp(-10*coef(model_binomial_lr)['Temp'])) - exp(-10*coef(model_binomial_lr)['Temp']), 2)) * 100`%. 

2.  Dalal, Fowlkes, and Hoadley (1989) chose to remove `pressure` from the model 
based on their likelihood ratio tests. Critically evaluate, using your test 
results and understanding of the question and data, whether `pressure` should be 
included in the model, or instead, `pressure` should not be included in the model. 
Your report needs to make a determination, argue why it is most appropriate choice, 
and make note of how (if at all) the model results are affected by the choice of 
including or excluding `pressure`.

For the test of *Pressure* with $H_0 : \beta_2 = 0$ vs. $H_\alpha : \beta_2 \neq 0$, 
The LRT statistic of *Pressure* is `r Anova(model_binomial_lr, test = 'LR')[2,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[2,3]`. Using the 
Type I Error rate $alpha = 0.05$, we fail to reject the null hypothesis. 
Therefore, there is a lack of evidence to include the feature variable *Pressure* 
in the model.

Thus, `Pressure` was probably removed in response to it lack of importance detected 
by the likelihood ratio test and because it does not add much systematic variation 
to the model (i.e. `Pressure` has unique values `r unique(df$Pressure)`). 
Our data set is very small and not representative. We do not have enough examples
for PSI 50 and 100. Potential problems could arise by excluding *Pressure* from
the model. If more data was available, pressure along with the interaction with
temperature might be helpful for explainability. If pressure is correlated with
temperature, and o-ring failure, by removing pressure, we would have introduced
omitted variable bias.

## Confidence Intervals

In our next model, we are just including temperature as our explanatory variable.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$ . 
With that, the model will be formulated as:-

$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i
$$

```{r, model temp, message=FALSE, warning=FALSE}
# Binomial Logit Model with just temperature
model_binomial_lr_temp <- glm(formula = O.ring / Number ~ Temp, 
               data = df, family = binomial(link = logit), weights = Number)

# Confidence interval
model_binomial_lr_temp_confint <- confint(model_binomial_lr_temp)
```

Binomial logistic regression model with temperature only is estimated in **Table 3** and shown here:
$$ logit\left(\hat{\pi}\right)= 5.085 - -0.116\text{Temp}$$


```{r lrt2, message=FALSE, warning=FALSE, echo=FALSE}
# Likelihood Ratio Test
model_binomial_lr_temp_lr <- Anova(model_binomial_lr_temp, test = 'LR')

anova_model_binomial_lr_temp_chisq_temp <- round(model_binomial_lr_temp_lr$`LR Chisq`[1],2)

anova_model_binomial_lr_temp_pr_temp <- round(model_binomial_lr_temp_lr$`Pr(>Chisq)`[1],2)
```

Again, we perform a likelihood ratio test using *Anova* and find $\chi^2$ value of `r anova_model_binomial_lr_temp_chisq_temp` corresponding to a P value of `r anova_model_binomial_lr_temp_pr_temp`, which is significant.


Finally, in our third logistic regression model, we use temperature and it's quadratic
term as explanatory features to predict the o-ring failure probability.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$. 
With that, the model will be formulated as:-

$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i + \beta_2 t_i ^ 2
$$



```{r, model with polynomial temp, message=FALSE, warning=FALSE}
# Binomial logistic regression with quadratic temperature term
model_binomial_lr_temp_poly <- glm(formula = O.ring / Number ~ Temp + I(Temp ^ 2), 
               data = df, family = binomial(link = logit), weights = Number)

# Confidence interval
model_binomial_lr_temp_poly_confint <- confint(model_binomial_lr_temp_poly)
```

```{r lrt3, message=FALSE, warning=FALSE, echo=FALSE}
# Log likelihood test
model_binomial_lr_temp_poly_lr <- Anova(model_binomial_lr_temp_poly, test = 'LR')

anova_model_binomial_lr_temp_poly_chisq_temp <- round(model_binomial_lr_temp_poly_lr$`LR Chisq`[1],2)
anova_model_binomial_lr_temp_poly_chisq_tempsq <- round(model_binomial_lr_temp_poly_lr$`LR Chisq`[2],2)

anova_model_binomial_lr_temp_poly_pr_temp <- round(model_binomial_lr_temp_poly_lr$`Pr(>Chisq)`[1],2)
anova_model_binomial_lr_temp_poly_pr_tempsq <- round(model_binomial_lr_temp_poly_lr$`Pr(>Chisq)`[2],2)
```

A likelihood ratio test is again ran on the polynomial model, with $\chi^2$ value of `r anova_model_binomial_lr_temp_poly_chisq_temp` for *Temperature* and P value of `r anova_model_binomial_lr_temp_poly_pr_temp`. For the quadratic term, there is a $\chi^2$ value of `r anova_model_binomial_lr_temp_poly_chisq_tempsq` for *Temperature* and P value of `r anova_model_binomial_lr_temp_poly_pr_tempsq`.

A comparison of all three models is presented in **Table 3**.

```{r models, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
cclist <- list(model_binomial_lr_confint, model_binomial_lr_temp_confint, model_binomial_lr_temp_poly_confint)

stargazer(
  model_binomial_lr, 
  model_binomial_lr_temp, 
  model_binomial_lr_temp_poly, 
  type='latex', 
  title='Comparison of Statistical Models to Predict Number of Failures', 
  header=FALSE,
  ci.custom=cclist,
  column.labels=c('Temp + Press','Temp Only','Temp w/ Quadratic')
)
```


We wanted to see if the quadratic temperature has any effect on predicting
o-ring failure and we do not see any statistical significance of it. So we are
not going to use this quadratic temperature feature in our model.

### Probability Figures

Using the binomial logistic regression model, we predicted the probability of failure using the second model (just *Temp*), and calculated the confidence intervals. Then, we calculated the expected number of O ring failures and plotted that vs. the temperature. This is shown in **Figure 2** 

```{r fig2, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Predicted Probability and Expected Number of O Ring Failures'}
t <- seq(31,81,1)
alpha=0.05
model_predict <- predict(object=model_binomial_lr_temp, 
                         newdata=data.frame(Temp=t), 
                         type='link', 
                         se=T)
CI_lower_linear <- model_predict$fit + qnorm(p=alpha/2)*model_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_predict$fit + qnorm(p=1-alpha/2)*model_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))

par(mfrow=c(1,2), oma=c(2,0,2,0))

##### pi with CI vs temp
plot(df$Temp, df$O.ring/df$Number,
     xlab= "Temperature (°F)",
     ylab=expression(pi),
     pch=20, cex=1.5,
     xlim=c(31,81),
     ylim=c(0,1), 
     sub="o-ring failure probability vs temp")
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
#title(expression(bold("o-ring failure probability vs temp")), adj=0)

##### Expected number of failures vs temp
plot(df$Temp, df$O.ring,
     xlab="Temperature (°F)",
     ylab="Expected Number\nof O-ring failures",
     pch=20, cex=1.5, 
     col="black",
     xlim=c(31,81),
     ylim=c(0,6),
     sub="Estimated o-ring failure vs temp")
## Binomial betas
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```
The confidence band is wider at temperatures lower than `r min(df$Temp)` °F 
because there are no observations below `r min(df$Temp)` °F.

### Probability of failure at 31°F

Using the models generated above, we predict the probability of a failure occurring on the day of the Challenger disaster, when launch temperature was estimated at 31°F.

```{r, extrapolate at 31 deg F}
# Data to extrapolate
predict.data <- data.frame(Temp = 31)
# Predict surface/link
predict.linear <- predict(object = model_binomial_lr_temp,
                          newdata = predict.data,
                          type = "link")
# Predict response 
predict.pi <- predict(object = model_binomial_lr_temp, 
                      newdata = predict.data,
                      type = "response")

pfail_31 <- data.frame(estimate=predict.pi, lower=CI_lower_pi[[1]], 
                       upper=CI_higher_pi[[1]])

```

The probability of O-ring failure at 31°F is `r round(predict.pi,3)` with a very wide 
confidence interval of `r round(c(CI_lower_pi[[1]], CI_higher_pi[[1]]),3)`. These are shown in **Table 4**.

```{r warning=FALSE, echo=FALSE, message=FALSE, results='asis'}
# Confidence interval for 31deg was computed above in part (c)
stargazer(head(pfail_31), summary=FALSE, header=FALSE, title='Estimated probability of failure with confidence bounds of fatal Challenger Flight.')
```


In order to infer the probability of O-ring failure at 31°F, we need to make 
the assumption that there is a linear relationship between the log odds of failure 
of an O-ring and Temperature. Because we use the binomial model, we do not meet 
the independence assumption. We also need to assume the model is built 
off a representative data set including records with both high and low 
temperatures, high and low pressure levels and similar data range is used for both
model training and inference. Otherwise the results will be uncertain and we can 
not trust the model's predictions.

## Bootstrap Confidence Intervals (30 points)

For the parametric bootstrap procedure, we have performed the below steps:

a. Setting a seed to make the results reproducible.

b. Estimate the proportion of o-ring failure using the estimated coefficients.

c. Instantiate a result data frame to store the results at 31 deg and 72 deg F.

d. Iterate of number of iterations and follow the steps:
   
   1. The original dataset is resampled with replacement to create a new dataset 
     $d$ of size 23.
  
   2. A vector of size 23 with outcome variable $O.ring2$ is generated using 
      rbinom function and using the sampled data and estimated $\hat\pi$ values.
      
   3. A new binomial logistic regression model is fitted with the resampled 
      dataset $d$ and outcomes $O.ring2$.    
     
   4. The predictions for 31 and 72 °F are found and the estimated probabilities 
      are saved to $results$.
     
e. Finally, 5% and 95% quantiles are derived from the predictions for 31 and 72 °F 
   and stored in $results$ and are reported as the 90% confidence intervals.

```{r, parametric bootstrap, message=FALSE, warning=FALSE}
# Set a seed
set.seed(123)

# Use pi estimated from the model
z = model_binomial_lr_temp$coefficients["(Intercept)"] + 
    model_binomial_lr_temp$coefficients["Temp"] * df$Temp
pi = exp(z)/(1+exp(z))

# Save the pi array to the dataframe
df$O.ring.pi <- pi

# Dataframe to populate with results
results <- data.frame(pred.31 = numeric(), pred.72 = numeric())
iterations = 10000
for (s in 1:iterations){
  
  I.sample <- sample(x = 1:nrow(df),
                     size = 23,
                     replace = T)
  # Populate d with samples
  d <- df[I.sample,]  
  # Simulate outcomes using rbinom
  O.ring2 <- rbinom(n=23, # sample size
                size=6, # number of trials
                prob=d$O.ring.pi) # probability
  
  # Stacking the simulated o.ring value with the sample data
  d <- data.frame(d, O.ring2)
  
  # Estimate model with rbinom bootstrap outcomes 
  # using the sampled data and sampled data
  mod <- glm(O.ring2/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = d)
  
  # Estimate confidence interval for temp = 31
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data,
                      type = "response")
  # Estimate confidence interval for temp = 72
  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data,
                      type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}

```

```{r quantiles, message=FALSE, echo=FALSE, warning=FALSE, results='asis'}
CI.31 <- quantile(results[,1], probs=c(0.05, 0.95))
CI.72 <- quantile(results[,2], probs=c(0.05, 0.95))
quantiles_df <- data.frame(temp=c(31,72), 
           lower.CI=c(CI.31[1],CI.72[1]), 
           upper.CI=c(CI.31[2],CI.72[2]))

stargazer(quantiles_df, summary=FALSE, header=FALSE, type='latex', title='Parametric Bootstrap Analysis Results')

```

```{r fig3, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Predicted Probability and Expected Number of O Ring Failures'}


# Plot histograms of confidence intervals
par(mfrow=c(1,2),mar=c(5,4,4,2))

# Histogram for temp = 31
hist(results[,1],
     breaks=50,
     col="lightskyblue3",
     xlab=expression(pi),
     xlim=c(0,1),
     main=' ')
mtext('(a) Probability of Failure at 31 °F', side = 3, padj = -2.5)

# Histogram for temp = 72
hist(results[,2],
     breaks=10,
     col="orangered",
     xlab=expression(pi),
     xlim=c(0,1), main=' ')
mtext('(b) Probability of Failure at 72 °F', side = 3, padj = -2.5)

```
The parametric bootstrap method to estimate confidence interval also shows that 
the confidence interval for 31°F is quite wide at `r round(CI.31[1],4)` and 
`r round(CI.31[2],4)`, due to the lack of data points at low temperatures. 
The confidence interval estimated for 72°F is much tighter, at `r round(CI.72[1],4)` 
and `r round(CI.72[2],4)`. 

Using `r iterations` iterations, through parametric bootstrap process, we have
generated the histogram of their probability of failures. The confidence intervals 
were taken as the 5th and 95th percentile of the distributions. The probability 
of failures for 31°F took on the full range of 0 and 1, and had a left skew 
towards 1. In contrast, the probability of failures for 72°F were more 
concentrated at the left end towards 0. 

## Alternative Specification 

A linear model was built to serve the same purpose of the logistic regression models and predict the number of O ring failures.

```{r, linear model, message=FALSE, warning=FALSE}
# Build Linear Model
model.linear <- lm(O.ring/Number ~ Temp, data = df)
```

```{r message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
# Summary
stargazer(model.linear, type="latex", header=FALSE, title='Linear Model to Predict Number of Failures')
```

```{r fig4, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Linear Regression Model'}
par(mfrow=c(2,2))
plot(model.linear)
hist(model.linear$residuals)
print(shapiro.test(sample(model.linear$residuals, size = 5000, replace = TRUE)))
jarque.bera.test(model.linear$residuals)
```
Estimated linear model is given below:-

$$ \cfrac{O.ring}{Number} = .616 - 0.008Temp$$
**Observations for the linear model:-**

 a. O-ring failure is inversely related with temperature, that is, with temperature 
    increases, the risk of O-ring failure decreases. 
 
 b. For each unit increase in temperature, the expected proportion of 
    O-rings that fail drops by `r coef(summary(model.linear))["Temp", 1]` with a 
    standard error of  `r coef(summary(model.linear))["Temp", 2]`, and *p*-value of 
    `r coef(summary(model.linear))["Temp", 4]`. 
    
*Linear regression model assumptions:*  

*a. IID*
   
   For our analysis, we used Challenger dataset and we used the entire population 
   to asses the Challenger disaster on January 28, 1986, and it satisfies the 
   identically distributed requirement. We do not have any evidence that one 
   particular flight record is related to other flight record, thus independent.

*b. No perfect colinearity*

   Our model contains only one explanatory variable, so we satisfy the condition.

*c. Linear Conditional Expectations*
   
   Based on the given data set, and plot for fitted vs residual, we do not see a 
   linear relationship. This assumption is not satisfied.

*d. Homoscadasticity*

   Homoscadasticity assumption is to have constant residual variance across the 
   range of explanatory variables. The occular test shows that this assumption 
   is not satisfied. 

*e. Normally Distributed Errors*

   The relationship between explanatory and the mean of outcome variable is linear. 
   Based on the qqplot, it’s obvious that the errors are not normally distributed: 
   we observe that the tail is significantly thinner than a normal distribution. 
   Both Jarque-Bera test and Shapiro-Wilk test denotes that the distribution on 
   the residuals distribution in question is significantly different from a normal 
   distribution beacsue of the prsence of outliers as seen the residual vs 
   fitted plot.

This linear model formulation also expects the proportion of o-ring failure is 
linearly related to the explanatory variables for all of their possible values,
which is not a valid assumption either.

We would choose binomial logistic regression model over the linear regression 
model for the below reasons:-

  a. The binomial logistic regression translates the problem statement in a clear
     proportion question. The response is the proportion of O-ring failure 
     (e.g. a proportion from 0 to 1).
     
  b. The linear model assumptions do not hold true here. Therefore, the linear 
     regression model is not applicable for this problem. On the other hand, 
     using the logistic regression, we do not need to assume a linear relationship 
     between the explanatory variable and response variable, normally distributed 
     residuals and residuals to have constant variance. As a result, binomial 
     logistic regression will be a better choice than linear regression.

# Conclusions (10 points)

Interpret the main result of your preferred model in terms of both odds and probability of failure. Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.

