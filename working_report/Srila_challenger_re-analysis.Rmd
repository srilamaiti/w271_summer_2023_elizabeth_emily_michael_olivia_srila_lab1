---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Elizabeth, Emily, Michael, Olivia and Srila"
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

\newpage

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
\end{abstract}
```

```{r load packages, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(sandwich)
library(lmtest)
library(MASS)
library(nnet)
library(cowplot)
library(corrplot)
library(knitr)
library(Hmisc)
library(gamlr)
library(finalfit)
library(stargazer)
library(tseries)
library(psych)
library(ggplot2) 
library(GGally) 
library(gridExtra) 
library(car) 
library(data.table) 
library(stargazer) 
library(skimr) 
library(mcprofile) 
library(dplyr) 
library(scatterplot3d) 
```

# Introduction

## Research question

# Data (20 points)

**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report. The Data Section of this report is worth 20 points.**

-   Conduct a thorough EDA of the data set.

    -   This should include both graphical and tabular analysis as taught in this course.
    -   Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals.

-   This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.

## Description

The data-set for analyzing the risk of O-ring failure in the Challenger Space Shuttle flight consists of twenty-three (23) out of the twenty-four (24) prior launches (one launch data is not available because the motors were lost at sea). The flight number increases by increment of one and corresponds to the incremental number of the flight (increases over date-time). 

In addition to the Flight number, variables for launch temperature (**Temperature (°F)**), leak test pressure (**Pressure (psi)**), and O-ring failure mechanism (**O-Ring**) are recorded. In the O-ring failure mechanism, a value of 1 indicates erosion while a value of 2 indicates blow-by. As was the case in (Siddhartha, 1989), because only one flight (Flight Number 14) experienced secondary O-ring distress, only primary O-ring distress is considered in the analysis. The full data set accompanied with comments on the nature of the O ring failure mechanisms is presented in **Table 1**.


```{r echo=FALSE, warning=FALSE, message=FALSE}
df <- read.csv('/Users/srilamaiti/Downloads/data/raw/challenger.csv')

summary(df)
head(df)
tail(df)
str(df)
dim(df)
which(is.na(df$Flight))
which(is.na(df$Temp))
which(is.na(df$Pressure))
which(is.na(df$O.ring))
which(is.na(df$Number))
lapply(df[c('Temp','Pressure','O.ring')], unique)

# Add some other columns
df$Mechanism <- ifelse(df$O.ring==1,"Erosion",ifelse(df$O.ring==2,"Blowby",""))
df$Comment <- ifelse(df$Flight==14, "Secondary O-ring Distress Noted", "")
```

```{r table1, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring","Mechanism","Comment")

stargazer(df[cols],type='latex', summary=FALSE, rownames=FALSE, 
          title='Twenty Three (23) Measured Flights Prior to Challenger Disaster',
          column.labels = c("Flight Number", "Temperature (°F)", "Pressure (psi)", "Primary O-Ring Failure Mechanism","Additional Comments"), 
          header=FALSE)

```

## Key Features

Evident in **Table 1** are some peculiarities corresponding to the features available in the data set. The first is that, over time, the leak test pressure changes. Dalal et al. explain that this was due to finding that the putty by itself could sustain 50 psi, so the leak test was increased to 100, and then to 200 psi to properly test for O ring capability. It's also noted, however, that this increased leak pressure may have contributed to blow holes, which can lead to erosion in the putty. The other major finding is the temperatures that were experienced leading up to the fatal Challenger flight, which was scheduled to launch with a temperature hovering around freezing (31 °F). As seen in **Table 2** explicitly, the minimum temperature experienced prior to the launch failure was 53°F. It's also seen in **Table 1** that this particular launch corresponded to the single example of secondary O ring distress and corresponded with primary O ring blow by. The presence of secondary O-ring failure is critical because it shows that this condition could lead to failure in both O rings and thus penetration of the engine gases.


```{r table2, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
cols <- c("Flight","Temp","Pressure","O.ring")

stargazer(df[cols], type='latex', summary=TRUE, header=FALSE,
          title="Baseline Statistics of Prior Flights to Challenger Disaster")

```

The temperature effect alluded to in the data set is confirmed in **Figure 1**, which is a panel figure comparing the features and their reported Pearson correlation coefficients. It's shown that the correlation between the the temperature component and the O-ring failures is -0.51, meaning that lower temperatures were considered to result in higher O-ring failures. Also note the correlation between *Pressure* and *Flight*, which is easily explained by the change in procedure over time. To a lessor degree also exists a correlation between *Pressure* and *O ring* which may indicate a potential pressure effect to explore. This is also supported by the theory discussed above that high leak test pressure can lead to putty erosion.

```{r pairsfig, fig.align="center", fig.width=8, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Panel Plot of Flight Variables with Pearson Correlation'}
cols <- c("Flight","Temp","Pressure","O.ring")

pairs.panels(
  df[cols], 
  method = "pearson", # correlation method
  hist.col = "#00AFBB",
  density = TRUE,  # show density plots
  ellipses = TRUE # show correlation ellipses
)

```

\newpage
# Analysis

## Reproducing Previous Analysis (10 points)

**Your analysis should address the following two questions. In your final submission, please remove this question prompt so that your report reads as a report.**

1.  Estimate the logistic regression model that the authors present in their report -- include the variables as linear terms in the model. Evaluate, using likelihood ratio tests, the statistical significance of each explanatory variable in the model. Evaluate, using the context and data understanding that you have created in the **Data** section of this report, the practical significance of each explanatory variable in the model.

From the exploratory data analysis, we found negative correlation between *Temp* and 
*O-ring* and positive correlation between *Pressure* and *O-ring*.
We want to include both explanatory features in our first logistic regression model 
to explore more.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$ and leak test pressure as $p_i$. 
With that, the binomial logistic regression model will be formulated as:-

$$
logit \left( \pi_i \right)  = log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 t_i + \beta_2 p_i
$$
```{r binomial logistic regression model}
model_binomial_lr <- glm(formula = O.ring / Number ~ Temp + Pressure, 
               data = df, family = binomial(link = logit), weights = Number)

# Summary of coefficients, standard errors and p-values
stargazer(model_binomial_lr, type="text")

# Confidence interval
cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))

# Log likelihood test
Anova(model_binomial_lr, test = 'LR')
```
Binomial logistic regression model is estimated as:
$$ logit\left(\hat{\pi}\right)= 2.520 - 0.098\text{Temp} + 0.008\text{Pressure} $$
```{r}
# Inverse odds ratio for binomial model
1/exp(summary(model_binomial_lr)$coefficients)
```
For the binomial logistic regression model, we have included two explanatory 
numeric variables *Temp* and *Pressure*. We have included them in the as-is form 
without any transformation. The outcome variables is a proportion, derived as 
the ratio of number o-ring failure to total number of o-ring present, which is 6. 

**Here are our observations for binomial logistic regression model:-** 

a. The results of the likelihood ratio test, we see that temperature appears to 
be statistically significant predictor on the proportion of o-ring failures. 
It is also showing the inverse relationship between o-ring failure proportion 
and the temperature. 

b. The pressure variable does not appear to be statistically significant predictor 
for predicting of o-ring failure proportion.

c. The coefficient of $t_i$ is estimated to be 
`r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,1]` 
with the 95% Wald confidence interval of `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,2]` to `r cbind(Estimate = coef(model_binomial_lr), confint(model_binomial_lr))[2,3]`, indicating that 
the decrease on the launch temperature would cause the increase on the odds 
for an O-ring to fail. 

d. In Anova test, involving feature variable of *Temp*, according to 
$H_0 : \beta_1 = 0$ and $H_\alpha : \beta_1 \neq 0$. 
The LRT statistic for *Temp* is `r Anova(model_binomial_lr, test = 'LR')[1,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[1,3]`. 

e. Using the Type I Error rate $alpha = 0.05$, we reject the null hypothesis and 
accept that the feature *Temp* is important to be included in the model with
other feature variable *Pressure* is included the model. 

f. For the test of *Pressure* with $H_0 : \beta_2 = 0$ vs. $H_\alpha : \beta_2 \neq 0$, 
The LRT statistic of *Pressure* is `r Anova(model_binomial_lr, test = 'LR')[2,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[2,3]`. Using the 
Type I Error rate $alpha = 0.05$, we fail to reject the null hypothesis. 
Therefore, there is a lack of evidence to include the feature variable *Pressure* 
in the model. 

g. With 10 deg F temperature drop increases the odds o-ring failure by approximately 
`r abs(round(floor(exp(-10*coef(model_binomial_lr)['Temp'])) - exp(-10*coef(model_binomial_lr)['Temp']), 2)) * 100`%. 

2.  Dalal, Fowlkes, and Hoadley (1989) chose to remove `pressure` from the model 
based on their likelihood ratio tests. Critically evaluate, using your test 
results and understanding of the question and data, whether `pressure` should be 
included in the model, or instead, `pressure` should not be included in the model. 
Your report needs to make a determination, argue why it is most appropriate choice, 
and make note of how (if at all) the model results are affected by the choice of 
including or excluding `pressure`.

For the test of *Pressure* with $H_0 : \beta_2 = 0$ vs. $H_\alpha : \beta_2 \neq 0$, 
The LRT statistic of *Pressure* is `r Anova(model_binomial_lr, test = 'LR')[2,1]` 
with a p-value of `r Anova(model_binomial_lr, test = 'LR')[2,3]`. Using the 
Type I Error rate $alpha = 0.05$, we fail to reject the null hypothesis. 
Therefore, there is a lack of evidence to include the feature variable *Pressure* 
in the model.

Thus, `Pressure` was probably removed in response to it lack of importance detected 
by the likelihood ratio test and because it does not add much systematic variation 
to the model (i.e. `Pressure` has unique values `r unique(df$Pressure)`). 
Our data set is very small and not representative. We do not have enough examples
for PSI 50 and 100. Potential problems could arise by excluding *Pressure* from
the model. If more data was available, pressure along with the interaction with
temperature might be helpful for explainability. If pressure is correlated with
temperature, and o-ring failure, by removing pressure, we would have introduced
omitted variable bias.

## Confidence Intervals (20 points)

No matter what you determined about using or dropping `pressure`, for this section 
begin by considering the simplified model $logit(\pi) = \beta_0 + \beta_1 Temp$, 
where $\pi$ is the probability of an O-ring failure. Complete the following:

1.  Estimate the logistic regression model.

In our next model, we are just including temperature as our explanatory variible.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$ . 
With that, the model will be formulated as:-

$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i
$$

```{r, model temp}
model_binomial_lr_temp <- glm(formula = O.ring / Number ~ Temp, 
               data = df, family = binomial(link = logit), weights = Number)

# Summary of coefficients, standard errors and p-values
stargazer(model_binomial_lr_temp, type="text")

# Confidence interval
cbind(Estimate = coef(model_binomial_lr_temp), confint(model_binomial_lr_temp))

# Log likelihood test
Anova(model_binomial_lr_temp, test = 'LR')
```

Binomial logistic regression model with temperature only is estimated as:
$$ logit\left(\hat{\pi}\right)= 5.085 - -0.116\text{Temp}$$

2.  Determine if a quadratic term is needed in the model for the temperature 
in this model.

In our third logistic regression model, we use temperature and it's quadratic
term as explanatory features to predict the o-ring failure probability.
For a given launch $i$, we denote the probability for an O-ring to fail as $\pi_i$, 
launch temperature as $t_i$. 
With that, the model will be formulated as:-

$$
logit \left( \pi_i \right)  = \beta_0 + \beta_1 t_i + \beta_2 t_i ^ 2
$$

```{r, model with polynomial temp}
model_binomial_lr_temp_poly <- glm(formula = O.ring / Number ~ Temp + I(Temp ^ 2), 
               data = df, family = binomial(link = logit), weights = Number)

# Summary of coefficients, standard errors and p-values
stargazer(model_binomial_lr_temp_poly, type="text")

# Confidence interval
cbind(Estimate = coef(model_binomial_lr_temp_poly), confint(model_binomial_lr_temp_poly))

# Log likelihood test
Anova(model_binomial_lr_temp_poly, test = 'LR')
```

We wanted to see if the quadratic temperature has any effect on predicting
o-ring failure and we do not see any statistical significance of it. So we are
not going to use this quadratic temperature feature in our model.

3.  Construct two plots:
4.  $\pi$ vs. Temp; and,
5.  Expected number of failures vs. Temp.

Specific requirements for these plots:

-   Use a temperature range of 31° to 81° on the x-axis even though the minimum 
temperature in the data set was 53°.\
-   Include the 95% Wald confidence interval bands for $\pi$ on the plot. 
Describe, in your analysis of these plots, why the bands much wider for lower 
temperatures than for higher temperatures?

```{r, predicted probability and o-ring failure with temprerature}
t <- seq(31,81,1)
alpha=0.05
model_predict <- predict(object=model_binomial_lr_temp, 
                         newdata=data.frame(Temp=t), 
                         type='link', 
                         se=T)
CI_lower_linear <- model_predict$fit + qnorm(p=alpha/2)*model_predict$se.fit
CI_lower_pi <- exp(CI_lower_linear)/(1+exp(CI_lower_linear))
CI_higher_linear <- model_predict$fit + qnorm(p=1-alpha/2)*model_predict$se.fit
CI_higher_pi <- exp(CI_higher_linear)/(1+exp(CI_higher_linear))

par(mfrow=c(1,2), oma=c(2,0,2,0))

##### pi with CI vs temp
plot(df$Temp, df$O.ring/df$Number,
     xlab= "Temperature (°F)",
     ylab=expression(pi),
     pch=20, cex=1.5,
     xlim=c(31,81),
     ylim=c(0,1), 
     sub="o-ring failure probability vs temp")
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x)), add=T)
lines(t, CI_lower_pi, lty = 'dashed')
lines(t, CI_higher_pi, lty = 'dashed')
#title(expression(bold("o-ring failure probability vs temp")), adj=0)

##### Expected number of failures vs temp
plot(df$Temp, df$O.ring,
     xlab="Temperature (°F)",
     ylab="Expected number of O-ring failures",
     pch=20, cex=1.5, 
     col="black",
     xlim=c(31,81),
     ylim=c(0,6),
     sub="Estimated o-ring failure vs temp")
## Binomial betas
# Betas
b0 <- model_binomial_lr_temp$coefficients[1]
b1 <- model_binomial_lr_temp$coefficients[2]
# Curve
curve(expr = exp(b0+b1*x)/(1+exp(b0+b1*x))*6, add=T)
```
The confidence band is wider at temperatures lower than `r min(df$Temp)` °F 
because there are no observations below `r min(df$Temp)` °F.

3.  The temperature was 31° at launch for the Challenger in 1986. Estimate the 
probability of an O-ring failure using this temperature, and compute a 
corresponding confidence interval. Discuss what assumptions need to be made in 
order to apply the inference procedures.

```{r, extrapolate at 31 deg F}
# Data to extrapolate
predict.data <- data.frame(Temp = 31)
# Predict surface/link
predict.linear <- predict(object = model_binomial_lr_temp,
                          newdata = predict.data,
                          type = "link")
# Predict response 
predict.pi <- predict(object = model_binomial_lr_temp, 
                      newdata = predict.data,
                      type = "response")
# Confidence interval for 31deg was computed above in part (c)
data.frame(estimate=predict.pi, lower=CI_lower_pi[[1]], upper=CI_higher_pi[[1]])
```
The probability of O-ring failure at 31°F is `r predict.pi` with a very wide 
confidence interval of `r c(CI_lower_pi[[1]], CI_higher_pi[[1]])`. 

In order to infer the probability of O-ring failure at 31°F, we need to make 
the assumption that there is a linear relationship between the log odds of failure 
of an O-ring and Temperature. Because we use the binomial model, we do not meet 
the independence assumption. We also need to assume the model is built 
off a representative data set including records with both high and low 
temperatures, high and low pressure levels and similar data range is used for both
model training and inference. Otherwise the results will be uncertain and we can 
not trust the model's predictions.

## Bootstrap Confidence Intervals (30 points)

Rather than relying on asymptotic properties, consider using a parametric 
bootstrap, as did Dalal, Fowlkes and Hoadley. To do this:

1.  Simulate a large number of data sets (n = 23 for each) by re-sampling with 
replacement from the data.
2.  Estimate a model for each dataset.
3.  Compute the effect at a specific temperature of interest.

To produce a confidence interval, the authors used the 0.05 and 0.95 observed 
quantiles from the simulated distribution as their 90% confidence interval limits.

Using the parametric bootstrap, compute 90% confidence intervals separately at 
each integer temperature between 10° and 100° Fahrenheit.

In this section, you should describe your process, justify such a process, and 
present your results in a way that is compelling for your reader.

##
For the parametric bootstrap procedure, we have performed the below steps:

a. Setting a seed to make the results reproducable.

b. Estimate the proportion of o-ring failure using the estimated coefficients.

c. Instantiate a result data frame to store the results at 31 deg and 72 deg F.

d. Iterate of number of iterations and follow the steps:
   
   1. The original dataset is resampled with replacement to create a new dataset 
     $d$ of size 23.
  
   2. A vector of size 23 with outcome variable $O.ring2$ is generated using 
      rbinom function and using the sampled data and estimated $\hat\pi$ values.
      
   3. A new binomial logistic regression model is fitted with the resampled 
      dataset $d$ and outcomes $O.ring2$.    
     
   4. The predictions for 31 and 72 °F are found and the estimated probabilities 
      are saved to $results$.
     
e. Finally, 5% and 95% quantiles are derived from the predictions for 31 and 72 °F 
   and stored in $results$ and is reported as the 90% confidence intervals.

```{r, parametric bootstrap}
# Set a seed
set.seed(123)

# Use pi estimated from the model
z = model_binomial_lr_temp$coefficients["(Intercept)"] + 
    model_binomial_lr_temp$coefficients["Temp"] * df$Temp
pi = exp(z)/(1+exp(z))

# Save the pi array to the dataframe
df$O.ring.pi <- pi

# Dataframe to populate with results
results <- data.frame(pred.31 = numeric(), pred.72 = numeric())
iterations = 10000
for (s in 1:iterations){
  
  I.sample <- sample(x = 1:nrow(df),
                     size = 23,
                     replace = T)
  # Populate d with samples
  d <- df[I.sample,]  
  # Simulate outcomes using rbinom
  O.ring2 <- rbinom(n=23, # sample size
                size=6, # number of trials
                prob=d$O.ring.pi) # probability
  
  # Stacking the simulated o.ring value with the sample data
  d <- data.frame(d, O.ring2)
  
  # Estimate model with rbinom bootstrap outcomes 
  # using the sampled data and sampled data
  mod <- glm(O.ring2/Number ~ Temp,
               family = binomial(link = logit),
               weights = Number,
               data = d)
  
  # Estimate confidence interval for temp = 31
  temp.31.data <- data.frame(Temp=31)
  temp.31 <- predict(object = mod, newdata = temp.31.data,
                      type = "response")
  # Estimate confidence interval for temp = 72
  temp.72.data <- data.frame(Temp=72)
  temp.72 <- predict(object = mod, newdata = temp.72.data,
                      type = "response")
  results <- results %>% add_row(pred.31 = temp.31, pred.72 = temp.72)  
}

CI.31 <- quantile(results[,1], probs=c(0.05, 0.95))
CI.72 <- quantile(results[,2], probs=c(0.05, 0.95))
data.frame(temp=c(31,72), 
           lower.CI=c(CI.31[1],CI.72[1]), 
           upper.CI=c(CI.31[2],CI.72[2]))

# Plot histograms of confidence intervals
par(mfrow=c(2,1),mar=c(5,4,4,2))

# Histogram for temp = 31
hist(results[,1],
     breaks=50,
     col="lightskyblue3",
     xlab=expression(pi),
     xlim=c(0,1),
     main=' ')
mtext('(a) Probability of Failure at 31 °F', side = 3, padj = -2.5)

# Histogram for temp = 72
hist(results[,2],
     breaks=10,
     col="orangered",
     xlab=expression(pi),
     xlim=c(0,1), main=' ')
mtext('(b) Probability of Failure at 72 °F', side = 3, padj = -2.5)

```
The parametric bootstrap method to estimate confidence interval also shows that 
the confidence interval for 31°F is quite wide at `r round(CI.31[1],4)` and 
`r round(CI.31[2],4)`, due to the lack of data points at low temperatures. 
The confidence interval estimated for 72°F is much tighter, at `r round(CI.72[1],4)` 
and `r round(CI.72[2],4)`. 

Using `r iterations` iterations, through parametric bootstrap process, we have
generated the histogram of their probability of failures. The confidence intervals 
were taken as the 5th and 95th percentile of the distributions. The probability 
of failures for 31°F took on the full range of 0 and 1, and had a left skew 
towards 1. In contrast, the probability of failures for 72°F were more 
concentrated at the left end towards 0. 

## Alternative Specification (10 points)

With the same set of explanatory variables in your final model, estimate a 
linear regression model. Explain the model results; conduct model diagnostic; 
and assess the validity of the model assumptions. Would you use the linear 
regression model or binary logistic regression in this case? Explain why.

```{r, linear model}
model.linear <- lm(O.ring/Number ~ Temp, data = df)

# Summary
stargazer(model.linear, type="text")
par(mfrow=c(2,2))
plot(model.linear)
hist(model.linear$residuals)
print(shapiro.test(sample(model.linear$residuals, size = 5000, replace = TRUE)))
jarque.bera.test(model.linear$residuals)
```
Estimated linear model is given below:-

$$ \cfrac{O.ring}{Number} = .616 - 0.008Temp$$
**Observations for the linear model:-**

 a. O-ring failure is inversely related with temperature, that is, with temperature 
    increases, the risk of O-ring failure decreases. 
 
 b. For each unit increase in temperature, the expected proportion of 
    O-rings that fail drops by `r coef(summary(model.linear))["Temp", 1]` with a 
    standard error of  `r coef(summary(model.linear))["Temp", 2]`, and *p*-value of 
    `r coef(summary(model.linear))["Temp", 4]`. 
    
*Linear regression model assumptions:*  

*a. IID*
   
   For our analysis, we used Challenger dataset and we used the entire population 
   to asses the Challenger disaster on January 28, 1986, and it satisfies the 
   identically distributed requirement. We do not have any evidence that one 
   particular flight record is related to other flight record, thus independent.

*b. No perfect colinearity*

   Our model contains only one explanatory variable, so we satisfy the condition.

*c. Linear Conditional Expectations*
   
   Based on the given data set, and plot for fitted vs residual, we do not see a 
   linear relationship. This assumption is not satisfied.

*d. Homoscadasticity*

   Homoscadasticity assumption is to have constant residual variance across the 
   range of explanatory variables. The occular test shows that this assumption 
   is not satisfied. 

*e. Normally Distributed Errors*

   The relationship between explanatory and the mean of outcome variable is linear. 
   Based on the qqplot, it’s obvious that the errors are not normally distributed: 
   we observe that the tail is significantly thinner than a normal distribution. 
   Both Jarque-Bera test and Shapiro-Wilk test denotes that the distribution on 
   the residuals distribution in question is significantly different from a normal 
   distribution beacsue of the prsence of outliers as seen the residual vs 
   fitted plot.

This linear model formulation also expects the proportion of o-ring failure is 
linearly related to the explanatory variables for all of their possible values,
which is not a valid assumption either.

We would choose binomial logistic regression model over the linear regression 
model for the below reasons:-

  a. The binomial logistic regression translates the problem statement in a clear
     proportion question. The response is the proportion of O-ring failure 
     (e.g. a proportion from 0 to 1).
     
  b. The linear model assumptions do not hold true here. Therefore, the linear 
     regression model is not applicable for this problem. On the other hand, 
     using the logistic regression, we do not need to assume a linear relationship 
     between the explanatory variable and response variable, normally distributed 
     residuals and residuals to have constant variance. As a result, binomial 
     logistic regression will be a better choice than linear regression.

# Conclusions (10 points)

Interpret the main result of your preferred model in terms of both odds and probability of failure. Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.


